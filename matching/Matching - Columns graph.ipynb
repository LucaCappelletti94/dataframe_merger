{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "from pyscipopt import Model, quicksum\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import ks_2samp\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vitamins_heuristic(name:str)->str:\n",
    "    \"\"\"Return given `name` with applied vitamin heuristic.\n",
    "        name:str, the name to which apply the heuristic.\n",
    "    \"\"\"\n",
    "    return re.sub(\"(vitamina?) ([a-z\\d]+)\", r\"\\1_\\2\", name)\n",
    "\n",
    "def fats_heuristic(name:str)->str:\n",
    "    \"\"\"Return given `name` with applied fats heuristic.\n",
    "        name:str, the name to which apply the heuristic.\n",
    "    \"\"\"\n",
    "    return name.replace(\":\", \"_\")\n",
    "\n",
    "def commons_heuristic(name:str)->str:\n",
    "    \"\"\"Return given `name` with applied commons heuristic.\n",
    "        name:str, the name to which apply the heuristic.\n",
    "    \"\"\"\n",
    "    drop = \"\\(mse\\)\", \"total[\\w]+\"\n",
    "    for d in drop:\n",
    "        name = re.sub(d, \"\", name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_heuristics(name):\n",
    "    \"\"\"Return given `name` with applied batch of heuristics.\n",
    "        name:str, the name to which apply the heuristics.\n",
    "    \"\"\"\n",
    "    heuristics = [\n",
    "        vitamins_heuristic,\n",
    "        fats_heuristic,\n",
    "        commons_heuristic\n",
    "    ]\n",
    "    for heuristic in heuristics:\n",
    "        name = heuristic(name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(corpus:List[str], texts:List[np.ndarray], heuristics:Callable[[np.ndarray],np.ndarray]=None):\n",
    "    \"\"\"Return tfidf vectorization using given `corpus` vector. If given, also applies heuristics element-wise.\n",
    "        corpus:List[str], list of texts to use as baseline for tfidf.\n",
    "        texts:List[np.ndarray], list of vectors to which apply heuristics and tfidf vectorization element-wise.\n",
    "        heuristics:Callable[[np.ndarray],np.ndarray], function to apply heauristics to texts.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(corpus)\n",
    "    if heuristics is not None:\n",
    "        texts = [heuristics(t) for t in texts]\n",
    "    return [vectorizer.transform(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_matrix_sum(A, B):\n",
    "    return np.nansum(\n",
    "        [A.reshape(A.shape[0], 1, A.shape[1]),\n",
    "         B.reshape(1, *B.shape)], axis=0)\n",
    "\n",
    "\n",
    "def pairwise_vector_sum(A, B):\n",
    "    Ar = A.reshape(-1, 1)\n",
    "    Br = B.reshape(1, B.size)\n",
    "    if A.size==1:\n",
    "        return np.nansum([Ar, Br.T], axis=0).T \n",
    "    return np.nansum([Ar, Br], axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def weighted_mse(A, B):\n",
    "    normalization = pairwise_matrix_sum(A, B)\n",
    "    difference = np.abs(pairwise_matrix_sum(A, -B))\n",
    "    mask = np.logical_or(np.isnan(normalization), normalization == 0)\n",
    "    return np.nanmean(np.divide(difference, normalization, where=~mask), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_differences(A: np.matrix, B: np.matrix) -> np.matrix:\n",
    "    \"\"\"Return weighted mean differences of given matrices.\"\"\"\n",
    "    if not A.shape[1] or not A.shape[1]:\n",
    "        return np.zeros((A.shape[1], B.shape[1]))\n",
    "    X, Y = np.nanmean(A.values, axis=0), np.nanmean(B.values, axis=0)\n",
    "    N, M = np.sum(pd.notna(A.values), axis=0), np.sum(pd.notna(B.values), axis=0)\n",
    "    result = np.abs((pairwise_vector_sum(X, -Y) * pairwise_vector_sum(N, -M)) /\n",
    "                  (pairwise_vector_sum(X, Y) + pairwise_vector_sum(N, M)))\n",
    "    return result/np.max(result)\n",
    "\n",
    "def pairwise_ks(A: List[np.ndarray], B: List[np.ndarray]) -> np.matrix:\n",
    "    \"\"\"Return weighted mean differences of given matrices.\"\"\"\n",
    "    return np.matrix([[ks_2samp(a, b)[1] for b in B] for a in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_to_type_groups(df):\n",
    "    types = df.columns.to_series().groupby(df.dtypes).groups\n",
    "    string = float64 = []\n",
    "    if np.dtype('O') in types:\n",
    "        string = types[np.dtype('O')]\n",
    "    if np.dtype('float64') in types:\n",
    "        float64 = types[np.dtype('float64')]\n",
    "        \n",
    "    return df[string], df[float64]\n",
    "\n",
    "def matrix_type_argsort(matrix: np.matrix, df: pd.DataFrame) -> np.matrix:\n",
    "    \"\"\"Return given `matrix` sorted using given `df` DataFrame types.\"\"\"\n",
    "    return matrix[np.argsort(list(zip(*sorted(zip(df.dtypes, range(len(df.dtypes))))))[1]),:]\n",
    "\n",
    "\n",
    "def double_argsort(floats: np.matrix, strings: np.matrix, df1: pd.DataFrame,\n",
    "                   df2: pd.DataFrame, fill) -> np.matrix:\n",
    "    \"\"\"Return combined matrix sorted in both axis using given dataframes types.\"\"\"\n",
    "    ground = np.full((df1.shape[1], df2.shape[1]), float(fill))\n",
    "    if np.all(floats.shape):\n",
    "        ground[:floats.shape[0], :floats.shape[1]] = floats\n",
    "    if np.all(strings.shape):\n",
    "        ground[-strings.shape[0]:, -strings.shape[1]:] = strings\n",
    "    return matrix_type_argsort(matrix_type_argsort(ground, df1).T, df2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mean(matrix):\n",
    "    if matrix.shape[0]==0 or matrix.shape[1]==0:\n",
    "        return matrix\n",
    "    #matrix[np.isclose(matrix, 0)] = np.nan\n",
    "    matrix[np.isclose(matrix, np.nanmax(matrix))] = np.nan\n",
    "    #matrix[np.isin(matrix, np.nanmax(matrix, axis=0))] = np.nan\n",
    "    #matrix[np.isin(matrix, np.nanmax(matrix, axis=1).reshape(-1,1))] = np.nan\n",
    "    matrix[np.isinf(matrix)] = np.nan\n",
    "    matrix[matrix>np.nanmean(matrix)] = np.nan\n",
    "    matrix[np.isnan(matrix)] = np.inf\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def filter_mask(matrix):\n",
    "    if matrix.shape[0]==0 or matrix.shape[1]==0:\n",
    "        return matrix\n",
    "    \n",
    "    return np.all([np.any(\n",
    "        [\n",
    "            matrix == np.min(matrix, axis=1).reshape(-1, 1), matrix == np.min(\n",
    "                matrix, axis=0)\n",
    "        ],\n",
    "        axis=0), ~np.isinf(matrix)], axis=0)\n",
    "\n",
    "\n",
    "def drop_nan(df):\n",
    "    return [df[c].dropna() for c in df]\n",
    "\n",
    "\n",
    "def column_values_mean_tfidf_distance(df1: pd.DataFrame,\n",
    "                                      df2: pd.DataFrame) -> np.matrix:\n",
    "    \"\"\"Return cosine distance of dataframes columns mean tfidf vectorization.\n",
    "        df1:pd.DataFrame, first string only dataframe.\n",
    "        df2:pd.DataFrame, second string only dataframe.\n",
    "    \"\"\"\n",
    "    if not df1.shape[1] or not df2.shape[1]:\n",
    "        return np.zeros((df1.shape[1], df2.shape[1]))\n",
    "    corpus = pd.concat(\n",
    "        [df[c].dropna() for df in [df1, df2] for c in df.columns])\n",
    "    vectors = tfidf(corpus, [*drop_nan(df1), *drop_nan(df2)])\n",
    "    mean_vectors = np.array([np.mean(v, axis=0) for v in vectors])\n",
    "    mean_vectors = mean_vectors.reshape(mean_vectors.shape[0],\n",
    "                                        mean_vectors.shape[-1])\n",
    "    return euclidean_distances(mean_vectors[:df1.shape[1]],\n",
    "                                mean_vectors[df1.shape[1]:])\n",
    "\n",
    "\n",
    "def tfidf_distance(corpus: List[str],\n",
    "                   A: np.ndarray,\n",
    "                   B: np.ndarray,\n",
    "                   heuristics: Callable = None) -> np.matrix:\n",
    "    \"\"\"Return cosine distance of dataframes columns mean tfidf vectorization.\n",
    "        corpus:List[str], list of texts to use as baseline for tfidf.\n",
    "        A:np.ndarray, vector to which apply heuristics and tfidf vectorization element-wise.\n",
    "        B:np.ndarray, vector to which apply heuristics and tfidf vectorization element-wise.\n",
    "        heuristics:Callable[[np.ndarray],np.ndarray], function to apply heauristics to texts.\n",
    "    \"\"\"\n",
    "    return euclidean_distances(*tfidf(corpus, [A, B], heuristics))\n",
    "\n",
    "\n",
    "def units_mask(A: np.ndarray, B: np.ndarray):\n",
    "    sep = \" | \"\n",
    "    unit_A, unit_B = [\n",
    "        np.array(\n",
    "            [None if len(c.split(sep)) == 1 else c.split(sep)[1] for c in v])\n",
    "        for v in [A, B]\n",
    "    ]\n",
    "    return unit_A[:, None] == unit_B\n",
    "\n",
    "\n",
    "def sub_incidence_matrix(corpus: List[str], A: pd.DataFrame, B: pd.DataFrame,\n",
    "                         heuristics: Callable[[np.ndarray], np.ndarray]):\n",
    "    strings, floats = list(\n",
    "        zip(columns_to_type_groups(A), columns_to_type_groups(B)))\n",
    "    return np.all(\n",
    "        [\n",
    "            np.all(\n",
    "                [\n",
    "                    #matrix_double_type_argsort(*[\n",
    "                    #    filter_mask(filter_mean(m)) for m in [\n",
    "                    #        weighted_mean_differences(*floats),\n",
    "                    #        column_values_mean_tfidf_distance(*strings)\n",
    "                    #    ]\n",
    "                    #], A, B),\n",
    "                    filter_mask(filter_mean(tfidf_distance(corpus, A.columns, B.columns, heuristics)))\n",
    "                ],\n",
    "                axis=0),\n",
    "            units_mask(A.columns, B.columns)\n",
    "        ],\n",
    "        axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incidence_matrix(corpus:List[str], dataframes:List[pd.DataFrame], heuristics:Callable):\n",
    "    n = sum([df.shape[1] for df in dataframes])\n",
    "    ground = np.zeros((n, n))\n",
    "    old = np.array([0, 0])\n",
    "    offset = 0\n",
    "    for i, df1 in enumerate(dataframes):\n",
    "        old[1] = offset\n",
    "        for j, df2 in enumerate(dataframes[i:], i):\n",
    "            if i == j:\n",
    "                matrix = np.eye(df1.shape[1])\n",
    "            else:\n",
    "                matrix = sub_incidence_matrix(corpus, df1, df2, heuristics)\n",
    "            x = slice(old[0], old[0] + df1.shape[1])\n",
    "            y = slice(old[1], old[1] + df2.shape[1])\n",
    "            ground[x, y] = matrix\n",
    "            ground[y, x] = matrix.T\n",
    "            old[1] += df2.shape[1]\n",
    "        offset += df1.shape[1]\n",
    "        old[0] += df1.shape[1]\n",
    "\n",
    "    return ground.astype(bool)#np.matrix(ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(dataframes: List[pd.DataFrame], heuristics: Callable):\n",
    "    corpus = heuristics(np.array([c for df in dataframes for c in df.columns]))\n",
    "    return incidence_matrix(corpus, dataframes, heuristics)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import clear_output\n",
    "def apply_matching(M, dataframes):\n",
    "    shapes = [df.shape[1] for df in dataframes]\n",
    "    columns = np.array([c for df in dataframes for c in df.columns])\n",
    "    m = np.repeat(np.arange(len(dataframes)), shapes)\n",
    "    for i, row in enumerate(M):\n",
    "        options = columns[row]\n",
    "        if np.sum(row) <= 1 or np.all(options== options[0]):\n",
    "            continue\n",
    "        print(\"Should I compact these? [number]/[n]\")\n",
    "        pprint(dict(zip(range(len(options)+1, 1), options)))\n",
    "        choice = input()\n",
    "        if choice != \"n\":\n",
    "            for j, v in enumerate(row):\n",
    "                if v:\n",
    "                    df = dataframes[m[j]]\n",
    "                    old = df.columns[j - sum(shapes[:m[j]])]\n",
    "                    dataframes[m[j]] = dataframes[m[j]].rename(\n",
    "                        columns={\n",
    "                            old:options[int(choice)]\n",
    "                        })\n",
    "        clear_output()\n",
    "        M[row] = 0 \n",
    "        M[:,row] = 0 \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../sanitized_csv/\"\n",
    "csvs = [\n",
    "    csv\n",
    "    for path, dirs, csvs in os.walk(path)\n",
    "    for csv in csvs if \"svizz\" not in csv\n",
    "]\n",
    "dataframes = [\n",
    "    pd.read_csv(\"{path}/{csv}\".format(path=path, csv=csv), index_col=[\"name\"])\n",
    "    for csv in csvs\n",
    "]\n",
    "columns = np.array([c for df in dataframes for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in greater\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: Mean of empty slice\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "M = matching(dataframes, np.vectorize(columns_heuristics, otypes=[str]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataframes = apply_matching(M, dataframes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "M0 = np.copy(M)\n",
    "for i, row in enumerate(M0):\n",
    "    if np.sum(row) > 1:\n",
    "        rows = np.where(row)\n",
    "        M0[rows] = 0 \n",
    "        M0[:,rows] = 0 \n",
    "        print(columns[rows])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
