{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import n2w\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances, manhattan_distances\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output, display\n",
    "from time import sleep\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_names(filename: str, path: str = \"csv\"):\n",
    "    return np.array(\n",
    "        pd.read_csv(\n",
    "            \"{path}/{filename}.csv\".format(path=path, filename=filename),\n",
    "            index_col=\"name\")[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_columns(filename: str, path: str = \"csv\"):\n",
    "    return np.array(\n",
    "        pd.read_csv(\n",
    "            \"{path}/{filename}.csv\".format(path=path, filename=filename),\n",
    "            index_col=\"name\").columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename: str, path: str = \"csv\"):\n",
    "    return pd.read_csv(filename, index_col=\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./fat classifier/fat_codes.json\", \"r\", encoding=\"UTF8\") as f:\n",
    "    fats = json.load(f)\n",
    "fat_keys = list(fats.keys())\n",
    "fat_keys.sort(key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vitamin_heuristics(name:str)->str:\n",
    "    return name.lower().replace(\"vitamina \", \"vitamina_\").replace(\"vit. \", \"vitamina_\")\n",
    "\n",
    "def percentage_heuristics(name:str)->str:\n",
    "    return name.replace(\"%\", \"percentuale\")\n",
    "\n",
    "def g_mg_heuristics(name:str)->str:\n",
    "    name = name.replace(\"(\", \" \").replace(\")\",\" \").replace(\"|\",\" \")\n",
    "    name = name.replace(\"mg\", \"milligrammi\").replace(\" g \", \"grammi\").replace(\"mcg\", \"microgrammi\")\n",
    "    return re.sub(\" g$\", \"grammi\", name)\n",
    "\n",
    "def fat_heuristic(name:str)->str:\n",
    "    global fats, fat_keys\n",
    "    \n",
    "    for key in fat_keys:\n",
    "        if key in name:\n",
    "            name.replace(key, \"{value} {key}\".format(value=fats[key][\"eng\"], key=key))\n",
    "    \n",
    "    if \":\" in name:\n",
    "        name = re.sub(r\"(C\\d+:\\d+)\\s(\\w)\", r\"\\1_\\2\", name)\n",
    "        for number in re.findall(\"\\d+\", name):\n",
    "            name = name.replace(number, n2w.convert(int(number)))\n",
    "        name = name.replace(\":\", \"_\")\n",
    "    \n",
    "    if \"÷\" in name:\n",
    "        name = name.replace(\"÷\", \"rate\")\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nutrients_heuristics(A: np.ndarray) -> np.ndarray:\n",
    "    return np.array([fat_heuristic(g_mg_heuristics(percentage_heuristics(vitamin_heuristics(a)))) for a in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncooked_heuristic(name:str)->str:\n",
    "    name = re.sub(\"crud[a-z]\", \"\", name)\n",
    "    name = re.sub(\"fresc[a-z]+\", \"\", name)\n",
    "    return name\n",
    "\n",
    "def with_heuristic(name:str)->str:\n",
    "    name = re.sub(\"senza \", \"senza_\", name)\n",
    "    name = re.sub(\"con \", \"con_\", name)\n",
    "    name = re.sub(\"non \", \"non_\", name)\n",
    "    return name\n",
    "    \n",
    "def cooking_heuristic(name:str)->str:\n",
    "    procedures = [\"tostat\", \"arrost\", \"affumicat\", \"fritt\"]\n",
    "    for procedure in procedures:\n",
    "        if procedure in name:\n",
    "            name = re.sub(procedure+\"[a-z]\", \"{procedure} cotto\".format(procedure=procedure), name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_food_heuristics(A: np.ndarray) ->np.ndarray:\n",
    "    heuristics = [uncooked_heuristic, cooking_heuristic, with_heuristic]\n",
    "    partial = A\n",
    "    for h in heuristics:\n",
    "        new_partial = np.array([h(a) for a in partial])\n",
    "        partial = new_partial\n",
    "    return partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(A: pd.DataFrame, B: pd.DataFrame,\n",
    "          threshold: float = 0.8) -> pd.DataFrame:\n",
    "    \n",
    "    common_columns = np.intersect1d(A.columns, B.columns)\n",
    "    \n",
    "    #print(len(common_columns))\n",
    "    \n",
    "    A_common = A[common_columns]\n",
    "    B_common = B[common_columns]\n",
    "    \n",
    "    A_T = A_common.transpose()\n",
    "    B_T = B_common.transpose()\n",
    "    \n",
    "    #Ac = A.columns#A_T.columns\n",
    "    #Bc = B.columns#B_T.columns\n",
    "    \n",
    "    Ac = A_T.columns\n",
    "    Bc = B_T.columns\n",
    "    \n",
    "    Ae = apply_food_heuristics(Ac)\n",
    "    Be = apply_food_heuristics(Bc)\n",
    "    \n",
    "    means_A = np.nanmean(A_T, axis=0)\n",
    "    means_B = np.nanmean(B_T, axis=0)\n",
    "    means_matrix = (\n",
    "        np.repeat(means_A.reshape(-1,1), means_B.size, axis=1) + \n",
    "        np.repeat(means_B.reshape(1, -1), means_A.size, axis=0)\n",
    "    )\n",
    "    mean_distance = manhattan_distances(means_A.reshape(-1,1), means_B.reshape(-1,1)) / means_matrix\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(np.concatenate([Ae, Be]))\n",
    "    X = vectorizer.transform(Ae)\n",
    "    Y = vectorizer.transform(Be)\n",
    "\n",
    "    tfidf_distances = euclidean_distances(X, Y)\n",
    "    tfidf_distances /= np.nanmax(tfidf_distances)\n",
    "    thr_mask = tfidf_distances > threshold\n",
    "    zero_row_mask = np.any(tfidf_distances == 0, axis=1)\n",
    "    zero_column_mask = np.any(tfidf_distances == 0, axis=0)\n",
    "    tfidf_distances[thr_mask] = np.inf\n",
    "    tfidf_distances[zero_row_mask] = np.inf\n",
    "    tfidf_distances[:, zero_column_mask] = np.inf\n",
    "    #np.fill_diagonal(tfidf_distances, np.inf)\n",
    "    # Fill upper triangle\n",
    "    #tfidf_distances[np.triu_indices(tfidf_distances.shape[0])] = np.inf\n",
    "    \n",
    "    distances = (tfidf_distances + mean_distance/np.nanmax(mean_distance))/2\n",
    "    distances[distances>threshold] = np.inf\n",
    "    \n",
    "    infinite_rows = np.all(np.logical_or(np.isnan(distances), np.isinf(distances)), axis=1)\n",
    "    infinite_cols = np.all(np.logical_or(np.isnan(distances), np.isinf(distances)), axis=0)\n",
    "    distances = distances[~infinite_rows]\n",
    "    distances = distances[:, ~infinite_cols]\n",
    "    \n",
    "    A1 = Ac[~infinite_rows]\n",
    "    B1 = Bc[~infinite_cols]\n",
    "    x_indices, y_indices = np.arange(A1.size), np.nanargmin(distances, axis=1)\n",
    "\n",
    "    x, y = A1[x_indices], B1[y_indices]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"Second\": y,\n",
    "        \"First\": x,\n",
    "        \"Values\": np.nanmin(distances, axis=1)\n",
    "    })\n",
    "    \n",
    "    df = df.sort_values(\"Values\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "yazio, total = load_df(\"csv/yazio.csv\"), load_df(\"csv/bda_crea_valori_alimentari.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Second</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>stomaco di maiale</td>\n",
       "      <td>stomaco di maiale cotto</td>\n",
       "      <td>0.126300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>farina d'orzo</td>\n",
       "      <td>farina di orzo</td>\n",
       "      <td>0.140015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>popcorn, cotti al microonde</td>\n",
       "      <td>popcorn al microonde</td>\n",
       "      <td>0.163084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>trippa di manzo, cruda</td>\n",
       "      <td>trippa di manzo cotta</td>\n",
       "      <td>0.181299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>patate, fecola</td>\n",
       "      <td>fecola di patate</td>\n",
       "      <td>0.199801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cavolo cappuccio rosso</td>\n",
       "      <td>cavolo cappuccio crudo</td>\n",
       "      <td>0.211954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>gamberetti</td>\n",
       "      <td>gamberetti scatola</td>\n",
       "      <td>0.215944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>pane tostato</td>\n",
       "      <td>pane bianco tostato</td>\n",
       "      <td>0.218656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>provolone</td>\n",
       "      <td>formaggio provolone</td>\n",
       "      <td>0.223408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>crauti</td>\n",
       "      <td>crauti scatola</td>\n",
       "      <td>0.229682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>felce arborea, cotta, senza sale</td>\n",
       "      <td>felce arborea cotta</td>\n",
       "      <td>0.234074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>prosciutto cotto</td>\n",
       "      <td>prosciutto crudo</td>\n",
       "      <td>0.236488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>soia, semi</td>\n",
       "      <td>semi di soia cotti</td>\n",
       "      <td>0.240874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>macinato di struzzo bollito</td>\n",
       "      <td>macinato di struzzo crudo</td>\n",
       "      <td>0.246123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>sesamo</td>\n",
       "      <td>farina di sesamo</td>\n",
       "      <td>0.246401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>wurstel</td>\n",
       "      <td>wurstel di maiale</td>\n",
       "      <td>0.249408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>coscia di tacchino arrosto</td>\n",
       "      <td>arrosto di tacchino</td>\n",
       "      <td>0.249688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>cumino</td>\n",
       "      <td>formaggio al cumino</td>\n",
       "      <td>0.250771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>mais, amido</td>\n",
       "      <td>amido di mais</td>\n",
       "      <td>0.251158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>cavolo cappuccio verde</td>\n",
       "      <td>cavolo cappuccio crudo</td>\n",
       "      <td>0.253847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>knorr vellutata di carciofi di gerusalemme</td>\n",
       "      <td>carciofi di gerusalemme</td>\n",
       "      <td>0.254017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>mais dolce, crudo</td>\n",
       "      <td>mais dolce bianco</td>\n",
       "      <td>0.256183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>noci pecan</td>\n",
       "      <td>torta di noci pecan</td>\n",
       "      <td>0.257508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>vino bianco da tavola</td>\n",
       "      <td>vino da tavola</td>\n",
       "      <td>0.258766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>torta alla crema</td>\n",
       "      <td>torta alla crema alla vaniglia</td>\n",
       "      <td>0.259938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>ghiande</td>\n",
       "      <td>ghiande essiccate</td>\n",
       "      <td>0.263832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tortellini, freschi</td>\n",
       "      <td>tortellini al formaggio</td>\n",
       "      <td>0.266237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>mandorle dolci, fresche</td>\n",
       "      <td>mandorle crude</td>\n",
       "      <td>0.266355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>bulgur, crudo</td>\n",
       "      <td>pane di bulgur</td>\n",
       "      <td>0.267835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>pollo arrosto</td>\n",
       "      <td>sovracoscia di pollo arrosto</td>\n",
       "      <td>0.273412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>nestle galak - barretta latte</td>\n",
       "      <td>latte</td>\n",
       "      <td>0.766705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>pere, candite</td>\n",
       "      <td>pere scatola</td>\n",
       "      <td>0.767521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>gomme da masticare, senza zucchero</td>\n",
       "      <td>gomma da masticare senza zucchero</td>\n",
       "      <td>0.769003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>miele, mcdonald's</td>\n",
       "      <td>pane al miele</td>\n",
       "      <td>0.769123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>nestle nero perugina - tavoletta pera</td>\n",
       "      <td>pera</td>\n",
       "      <td>0.769189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>the nestea al limone</td>\n",
       "      <td>pollo al limone</td>\n",
       "      <td>0.769584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>galbusera cerealicosì</td>\n",
       "      <td>galbusera biscotti</td>\n",
       "      <td>0.769752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>caciotta romana di pecora</td>\n",
       "      <td>lattuga romana</td>\n",
       "      <td>0.770449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>nestle nescafè - cappuccino decaffeinato</td>\n",
       "      <td>caffe decaffeinato</td>\n",
       "      <td>0.770541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>galbusera buonicosì</td>\n",
       "      <td>galbusera biscotti</td>\n",
       "      <td>0.771073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>galbusera colcuore cracker</td>\n",
       "      <td>cracker</td>\n",
       "      <td>0.771782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>galbusera tra</td>\n",
       "      <td>galbusera biscotti</td>\n",
       "      <td>0.773470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>bombolone lievitato al miele, con glassa</td>\n",
       "      <td>pane al miele</td>\n",
       "      <td>0.773896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>galbusera duecosì</td>\n",
       "      <td>galbusera biscotti</td>\n",
       "      <td>0.774580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>galbusera risosuriso cracker</td>\n",
       "      <td>cracker</td>\n",
       "      <td>0.775410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>aringa dell'atlantico</td>\n",
       "      <td>olio di aringa</td>\n",
       "      <td>0.776019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>gambero, cotto</td>\n",
       "      <td>gambero dacqua dolce crudo</td>\n",
       "      <td>0.780880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>pane alle olive</td>\n",
       "      <td>olive</td>\n",
       "      <td>0.780990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>granarolo - alta qualità yogurt ai cereali - y...</td>\n",
       "      <td>barretta ai cereali e riso</td>\n",
       "      <td>0.782930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>nori, secca</td>\n",
       "      <td>alghe nori</td>\n",
       "      <td>0.783705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>litchi, secchi</td>\n",
       "      <td>litchi essiccati</td>\n",
       "      <td>0.785517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>cerfoglio secco</td>\n",
       "      <td>cerfoglio essiccato</td>\n",
       "      <td>0.786567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>nestle nero perugina - tavoletta arancia</td>\n",
       "      <td>arancia</td>\n",
       "      <td>0.789284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>galbusera leggeri plus</td>\n",
       "      <td>galbusera biscotti</td>\n",
       "      <td>0.789882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>agnesi pasta di semola gli speciali forno - la...</td>\n",
       "      <td>lasagne</td>\n",
       "      <td>0.790170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>olive nere</td>\n",
       "      <td>olive</td>\n",
       "      <td>0.790450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>barilla - mulino bianco fette biscottate dorate</td>\n",
       "      <td>fette biscottate kamut germinal</td>\n",
       "      <td>0.795232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>nestle after eight - astuccio formato 300 grammi</td>\n",
       "      <td>after eight nestle</td>\n",
       "      <td>0.795257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>nestle - baby granulato gusto biscotto</td>\n",
       "      <td>biscotto gelato</td>\n",
       "      <td>0.797827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>nestle - motta la cremeria - biscotto</td>\n",
       "      <td>biscotto gelato</td>\n",
       "      <td>0.798520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1287 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  First  \\\n",
       "718                                   stomaco di maiale   \n",
       "525                                       farina d'orzo   \n",
       "748                         popcorn, cotti al microonde   \n",
       "115                              trippa di manzo, cruda   \n",
       "94                                       patate, fecola   \n",
       "30                               cavolo cappuccio rosso   \n",
       "1028                                         gamberetti   \n",
       "338                                        pane tostato   \n",
       "262                                           provolone   \n",
       "529                                              crauti   \n",
       "973                    felce arborea, cotta, senza sale   \n",
       "513                                    prosciutto cotto   \n",
       "218                                          soia, semi   \n",
       "1033                        macinato di struzzo bollito   \n",
       "782                                              sesamo   \n",
       "131                                             wurstel   \n",
       "77                           coscia di tacchino arrosto   \n",
       "694                                              cumino   \n",
       "330                                         mais, amido   \n",
       "161                              cavolo cappuccio verde   \n",
       "783          knorr vellutata di carciofi di gerusalemme   \n",
       "93                                    mais dolce, crudo   \n",
       "73                                           noci pecan   \n",
       "23                                vino bianco da tavola   \n",
       "732                                    torta alla crema   \n",
       "797                                             ghiande   \n",
       "69                                  tortellini, freschi   \n",
       "141                             mandorle dolci, fresche   \n",
       "1201                                      bulgur, crudo   \n",
       "104                                       pollo arrosto   \n",
       "...                                                 ...   \n",
       "812                       nestle galak - barretta latte   \n",
       "254                                       pere, candite   \n",
       "744                  gomme da masticare, senza zucchero   \n",
       "1093                                  miele, mcdonald's   \n",
       "1159              nestle nero perugina - tavoletta pera   \n",
       "1110                               the nestea al limone   \n",
       "887                               galbusera cerealicosì   \n",
       "507                           caciotta romana di pecora   \n",
       "1276           nestle nescafè - cappuccino decaffeinato   \n",
       "891                                 galbusera buonicosì   \n",
       "1260                         galbusera colcuore cracker   \n",
       "1140                                      galbusera tra   \n",
       "1104           bombolone lievitato al miele, con glassa   \n",
       "1025                                  galbusera duecosì   \n",
       "992                        galbusera risosuriso cracker   \n",
       "608                               aringa dell'atlantico   \n",
       "936                                      gambero, cotto   \n",
       "138                                     pane alle olive   \n",
       "787   granarolo - alta qualità yogurt ai cereali - y...   \n",
       "171                                         nori, secca   \n",
       "984                                      litchi, secchi   \n",
       "1149                                    cerfoglio secco   \n",
       "1139           nestle nero perugina - tavoletta arancia   \n",
       "1241                             galbusera leggeri plus   \n",
       "1105  agnesi pasta di semola gli speciali forno - la...   \n",
       "605                                          olive nere   \n",
       "1012    barilla - mulino bianco fette biscottate dorate   \n",
       "779    nestle after eight - astuccio formato 300 grammi   \n",
       "940              nestle - baby granulato gusto biscotto   \n",
       "698               nestle - motta la cremeria - biscotto   \n",
       "\n",
       "                                 Second    Values  \n",
       "718             stomaco di maiale cotto  0.126300  \n",
       "525                      farina di orzo  0.140015  \n",
       "748                popcorn al microonde  0.163084  \n",
       "115               trippa di manzo cotta  0.181299  \n",
       "94                     fecola di patate  0.199801  \n",
       "30               cavolo cappuccio crudo  0.211954  \n",
       "1028                 gamberetti scatola  0.215944  \n",
       "338                 pane bianco tostato  0.218656  \n",
       "262                 formaggio provolone  0.223408  \n",
       "529                      crauti scatola  0.229682  \n",
       "973                 felce arborea cotta  0.234074  \n",
       "513                    prosciutto crudo  0.236488  \n",
       "218                  semi di soia cotti  0.240874  \n",
       "1033          macinato di struzzo crudo  0.246123  \n",
       "782                    farina di sesamo  0.246401  \n",
       "131                   wurstel di maiale  0.249408  \n",
       "77                  arrosto di tacchino  0.249688  \n",
       "694                 formaggio al cumino  0.250771  \n",
       "330                       amido di mais  0.251158  \n",
       "161              cavolo cappuccio crudo  0.253847  \n",
       "783             carciofi di gerusalemme  0.254017  \n",
       "93                    mais dolce bianco  0.256183  \n",
       "73                  torta di noci pecan  0.257508  \n",
       "23                       vino da tavola  0.258766  \n",
       "732      torta alla crema alla vaniglia  0.259938  \n",
       "797                   ghiande essiccate  0.263832  \n",
       "69              tortellini al formaggio  0.266237  \n",
       "141                      mandorle crude  0.266355  \n",
       "1201                     pane di bulgur  0.267835  \n",
       "104        sovracoscia di pollo arrosto  0.273412  \n",
       "...                                 ...       ...  \n",
       "812                               latte  0.766705  \n",
       "254                        pere scatola  0.767521  \n",
       "744   gomma da masticare senza zucchero  0.769003  \n",
       "1093                      pane al miele  0.769123  \n",
       "1159                               pera  0.769189  \n",
       "1110                    pollo al limone  0.769584  \n",
       "887                  galbusera biscotti  0.769752  \n",
       "507                      lattuga romana  0.770449  \n",
       "1276                 caffe decaffeinato  0.770541  \n",
       "891                  galbusera biscotti  0.771073  \n",
       "1260                            cracker  0.771782  \n",
       "1140                 galbusera biscotti  0.773470  \n",
       "1104                      pane al miele  0.773896  \n",
       "1025                 galbusera biscotti  0.774580  \n",
       "992                             cracker  0.775410  \n",
       "608                      olio di aringa  0.776019  \n",
       "936          gambero dacqua dolce crudo  0.780880  \n",
       "138                               olive  0.780990  \n",
       "787          barretta ai cereali e riso  0.782930  \n",
       "171                          alghe nori  0.783705  \n",
       "984                    litchi essiccati  0.785517  \n",
       "1149                cerfoglio essiccato  0.786567  \n",
       "1139                            arancia  0.789284  \n",
       "1241                 galbusera biscotti  0.789882  \n",
       "1105                            lasagne  0.790170  \n",
       "605                               olive  0.790450  \n",
       "1012    fette biscottate kamut germinal  0.795232  \n",
       "779                  after eight nestle  0.795257  \n",
       "940                     biscotto gelato  0.797827  \n",
       "698                     biscotto gelato  0.798520  \n",
       "\n",
       "[1287 rows x 3 columns]"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match(total, yazio, threshold=0.8)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "path = \"csv/bda_crea_valori_alimentari.csv\"\n",
    "\n",
    "dataset = pd.read_csv(path, index_col=\"name\")\n",
    "\n",
    "def mse(tasks):\n",
    "    found = []\n",
    "    for task in tasks:\n",
    "        e1, e2 = task\n",
    "        if e1.replace(\",\", \"\") == e2.replace(\",\", \"\") and e1 != e2:\n",
    "            found.append((e1, e2))\n",
    "    return found\n",
    "\n",
    "print(\"Generating jobs.\")\n",
    "jobs = [[(e1, e2) for e2 in tqdm(dataset.index, leave=False)] for e1 in tqdm(dataset.index)]\n",
    "print(\"Starting jobs.\")\n",
    "with Pool(cpu_count()) as p:   \n",
    "    found = list(tqdm(p.imap(mse, jobs), total=len(jobs)))\n",
    "            \n",
    "with open(\"match_found.json\", \"w\") as f:\n",
    "    json.dump(found, f)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for df_path in tqdm(\n",
    "    [\"csv/crea.csv\", \"csv/bda.csv\", \"csv/valori_alimentari.csv\"]):\n",
    "    df = load_df(df_path)\n",
    "    for c in tqdm(df.columns, leave=False):\n",
    "        name, unit = c.split(\" | \")\n",
    "        if unit in [\"mg\", \"mcg\"]:\n",
    "            if unit == \"mg\":\n",
    "                factor = 1e-3\n",
    "            elif unit == \"mcg\":\n",
    "                factor = 1e-6\n",
    "            df[c.replace(\"| {unit}\".format(unit=unit), \"| g\")] = df[c] * factor\n",
    "            df = df.drop(columns=[c])\n",
    "            df.to_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_aliases_path = \"food_aliases.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_bda = total\n",
    "vn = yazio\n",
    "all_columns = list(set([*crea_bda.columns, *vn.columns]))\n",
    "common_indices = list(set(crea_bda.index) & set(vn.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_only_indices = list(set(crea_bda.index) - set(vn.index))\n",
    "bda_only_indices = list(set(vn.index) - set(crea_bda.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_dataframe_a = pd.DataFrame([], columns=all_columns)\n",
    "fusion_dataframe_b = pd.DataFrame([], columns=all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_dataframe_a = fusion_dataframe_a.append(crea_bda.loc[common_indices])\n",
    "fusion_dataframe_b = fusion_dataframe_b.append(vn.loc[common_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_ndarray = np.stack([fusion_dataframe_a, fusion_dataframe_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_mask = np.all(np.isnan(fusion_ndarray.astype(float)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_ndarray[:, nan_mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fusion = np.nanmean(fusion_ndarray, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fusion[nan_mask] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_common_final = pd.DataFrame(mean_fusion, columns=fusion_dataframe_a.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_common_final.index = common_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_common_final_plus_crea = fusion_common_final.append(crea_bda.loc[crea_only_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_common_final_plus_crea_plus_bda = fusion_common_final_plus_crea.append(vn.loc[bda_only_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_common_final_plus_crea_plus_bda.index.name = \"name\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "fusion_common_final_plus_crea_plus_bda.to_csv(\"csv/bda_crea_valori_alimentari.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4824, 197)"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fusion_common_final_plus_crea_plus_bda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "crea_common = crea.loc[common_indices]\n",
    "bda_common = bda.loc[common_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_common_final_plus_crea_plus_bda[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_common = np.nanmean([crea_common, bda_common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_header(df, old, new, path):\n",
    "    df[new] = df[old]\n",
    "    df = df.drop(columns=[old])\n",
    "    df.to_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_index(df, old, new, path, newnew=\"\"):\n",
    "    global food_aliases_path\n",
    "    with open(food_aliases_path, \"r\") as f:\n",
    "        food_aliases = json.load(f)\n",
    "\n",
    "    if old in food_aliases:\n",
    "        food_aliases[old].append(new)\n",
    "    elif new in food_aliases:\n",
    "        food_aliases[new].append(old)\n",
    "    else:\n",
    "        food_aliases[new] = [old]\n",
    "\n",
    "    with open(food_aliases_path, \"w\") as f:\n",
    "        json.dump(food_aliases, f)\n",
    "\n",
    "    if old in df.index and new in df.index:\n",
    "        drop_new = False\n",
    "        if not newnew:\n",
    "            drop_new = True\n",
    "            newnew = new\n",
    "        df.loc[newnew] = np.nanmean(df.loc[[old, new]], axis=0)\n",
    "        df = df.drop(old)\n",
    "        if drop_new:\n",
    "            df = df.drop(new)\n",
    "    else:\n",
    "        df = df.rename(index={old: new})\n",
    "    df.to_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "path_df2, path_df1 = \"csv/bda_crea_valori_alimentari.csv\", \"csv/yazio.csv\"\n",
    "done = False\n",
    "start = 0\n",
    "while True:\n",
    "    df1, df2 = load_df(path_df1), load_df(path_df2)\n",
    "    r = match(df1, df2, threshold=0.8)\n",
    "    n = len(r)\n",
    "    if not start:\n",
    "        start = n\n",
    "\n",
    "    common_columns = np.intersect1d(df1.columns, df2.columns)\n",
    "    df1t = df1[common_columns].transpose()\n",
    "    df2t = df2[common_columns].transpose()\n",
    "\n",
    "    n = 0\n",
    "    for first, second, value in r.values:\n",
    "        if first == second:\n",
    "            continue\n",
    "        n += 1\n",
    "\n",
    "    if not n:\n",
    "        break\n",
    "\n",
    "    #print(\"I have found {n} possible matches. \".format(n=n))\n",
    "    #sleep(0.75)\n",
    "\n",
    "    for first, second, value in r.values:\n",
    "\n",
    "        if first not in df1t or second not in df2t:\n",
    "            continue\n",
    "\n",
    "        done = False\n",
    "        if first == second:\n",
    "            n -= 1\n",
    "            continue\n",
    "\n",
    "        if \"agnesi\" in first:\n",
    "            n -= 1\n",
    "            continue\n",
    "        # Homogeneus constraint:\n",
    "        \n",
    "        homogeneus = [\"pollo\",\"tacchino\",\"struzzo\",\"mais\",\"peperone\",\"coniglio\",\"radicchio\",\"hamburger\"] # maiale no couz suino, vitello no couz manzo\n",
    "        \n",
    "        for kind in homogeneus:\n",
    "            if (kind in first and kind not in second) or (kind not in first and kind in second):\n",
    "                keys = \"{two} {one}\".format(\n",
    "                    one=first, two=second), \"{one} {two}\".format(\n",
    "                        one=first, two=second)\n",
    "                skipped.setdefault(keys[0], 0)\n",
    "                skipped.setdefault(keys[1], 0)\n",
    "                skipped[keys[0]] += 1\n",
    "                skipped[keys[1]] += 1\n",
    "                skipped[keys[0]] += 5\n",
    "                skipped[keys[1]] += 5\n",
    "        \n",
    "        # End Homogeneus\n",
    "        # Opposites should not be considered\n",
    "        opposites = [\n",
    "            (\"crudo\",\"cotto\"),\n",
    "        ]\n",
    "        \n",
    "        for first_key, second_key in opposites:\n",
    "            if (first_key in first and second_key in second) or (first_key in second and second_key in first):\n",
    "                    \n",
    "                keys = \"{two} {one}\".format(\n",
    "                    one=first, two=second), \"{one} {two}\".format(\n",
    "                        one=first, two=second)\n",
    "                skipped.setdefault(keys[0], 0)\n",
    "                skipped.setdefault(keys[1], 0)\n",
    "                skipped[keys[0]] += 1\n",
    "                skipped[keys[1]] += 1\n",
    "                skipped[keys[0]] += 5\n",
    "                skipped[keys[1]] += 5\n",
    "        # End Opposites\n",
    "        \n",
    "        skip = False\n",
    "        for name in []:\n",
    "            if bool(re.findall(name, first)) == bool(re.findall(name, second)):\n",
    "                n -= 1\n",
    "                skip = True\n",
    "                break\n",
    "        if skip:\n",
    "            continue\n",
    "\n",
    "        keys = \"{two} {one}\".format(\n",
    "            one=first, two=second), \"{one} {two}\".format(\n",
    "                one=first, two=second)\n",
    "        if keys[0] in skipped and skipped[keys[0]] > 2 or keys[1] in skipped and skipped[keys[1]] > 2:\n",
    "            n -= 1\n",
    "            continue\n",
    "        mse = np.nanmean((df2t[second] - df1t[first])**2)\n",
    "        if mse > 300:\n",
    "            n -= 1\n",
    "            continue\n",
    "        while True:\n",
    "            clear_output()\n",
    "            print(\"Forza! Ne rimangon solo {n}! Sei al {perc:.1f}%!\".format(\n",
    "                n=n, perc=(1 - n / start) * 100))\n",
    "            print(\n",
    "                \"I found\\n\\033[1m{first}\\033[0m\\n\\033[1m{second}\\033[0m \\nShould I merge them?\".\n",
    "                format(first=first, second=second))\n",
    "\n",
    "            print(\n",
    "                \"Their mse is: {mse:.4f}, with means {mean_1:.4f} and {mean_2:.4f}\".\n",
    "                format(\n",
    "                    mse=mse,\n",
    "                    mean_1=np.nanmean(df1t[first]),\n",
    "                    mean_2=np.nanmean(df2t[second])))\n",
    "            if mse < 0.1:\n",
    "                print(\"\\033[1mMSE BASSISSIMOOOO!\\033[0m\")\n",
    "            elif mse < 1:\n",
    "                print(\"\\033[1mMSE BASSO!\\033[0m\")\n",
    "            elif mse < 5:\n",
    "                print(\"\\033[1mMSE INTERESSANTE!\\033[0m\")\n",
    "            elif mse > 1000:\n",
    "                print(\"\\033[1mMSE ALTO!\\033[0m\")\n",
    "            inp = input(\"[y/n/d/del/ren]\")\n",
    "            if inp == \"y\":\n",
    "                while True:\n",
    "                    h = input(\"Which header should I use? [1/2/0/header] \")\n",
    "                    if h == \"1\":\n",
    "                        df1 = replace_index(df1, first, second, path_df1)\n",
    "                        done = True\n",
    "                        break\n",
    "                    elif h == \"2\":\n",
    "                        df2 = replace_index(df2, second, first, path_df2)\n",
    "                        done = True\n",
    "                        break\n",
    "                    elif h == \"0\":\n",
    "                        break\n",
    "                    elif not h:\n",
    "                        print(\"What did you mean? Please retry.\")\n",
    "                    else:\n",
    "                        choice = input(\n",
    "                            \"Should i use '{header}'? [y/n]\".format(header=h))\n",
    "                        if choice == \"y\":\n",
    "                            df2 = replace_index(df2, second, first, path_df2,\n",
    "                                                h)\n",
    "                            done = True\n",
    "                            break\n",
    "                        elif choice == \"n\":\n",
    "                            print(\"Okay restarting\")\n",
    "                        else:\n",
    "                            print(\"What did you mean? Please retry.\")\n",
    "                if done:\n",
    "                    break\n",
    "            elif inp == \"d\":\n",
    "                data = pd.DataFrame({\n",
    "                    first: df1t[first],\n",
    "                    second: df2t[second],\n",
    "                    \"diff\": np.abs(df1t[first] - df2t[second])\n",
    "                })\n",
    "\n",
    "                display(data.sort_values(\"diff\", ascending=False)[:10])\n",
    "                input(\"Press any key to continue\")\n",
    "            elif inp == \"n\" or not inp:\n",
    "                skipped.setdefault(keys[0], 0)\n",
    "                skipped.setdefault(keys[1], 0)\n",
    "                skipped[keys[0]] += 1\n",
    "                skipped[keys[1]] += 1\n",
    "                if inp == \"n\":\n",
    "                    skipped[keys[0]] += 5\n",
    "                    skipped[keys[1]] += 5\n",
    "                print(\"Ok, leaving it be.\")\n",
    "                break\n",
    "            elif inp == \"ren\":\n",
    "                new_header = input(\"Enter your new header.\")\n",
    "                while True:\n",
    "                    h = input(\"Which one do you want to rename? [1/2]\")\n",
    "                    if h == \"1\":\n",
    "                        df1.rename(index={first: new_header}).to_csv(path_df1)\n",
    "                        done = True\n",
    "                        break\n",
    "                    elif h == \"2\":\n",
    "                        df2.rename(index={second: new_header}).to_csv(path_df2)\n",
    "                        done = True\n",
    "                        break\n",
    "                    elif h == \"0\":\n",
    "                        break\n",
    "                if done:\n",
    "                    break\n",
    "            elif inp == \"del\":\n",
    "                while True:\n",
    "                    h = input(\"Which one should I delete? [1/2/0]\")\n",
    "                    if h == \"1\":\n",
    "                        df1.drop(first).to_csv(path_df1)\n",
    "                        done = True\n",
    "                        break\n",
    "                    elif h == \"2\":\n",
    "                        df2.drop(second).to_csv(path_df2)\n",
    "                        done = True\n",
    "                        break\n",
    "                    elif h == \"0\":\n",
    "                        break\n",
    "                if done:\n",
    "                    break\n",
    "            else:\n",
    "                print(\"What did you mean? Please retry.\")\n",
    "        if done:\n",
    "            break\n",
    "    clear_output()\n",
    "    if not done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"match_found.json\", \"r\") as f:\n",
    "    match_found = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_matches = [match for matches in match_found if matches for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['albicocche sciroppate', 'albicocche, sciroppate'],\n",
       " ['fette biscottate, integrali', 'fette biscottate integrali'],\n",
       " ['fette biscottate integrali', 'fette biscottate, integrali'],\n",
       " ['albicocche, sciroppate', 'albicocche sciroppate']]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
